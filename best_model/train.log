Arguments are...
log_dir: ./test_run
sdf_dir: data/intra_gsm_clean
split_path: data/intra_gsm_clean/splits/split0.npy
n_epochs: 100
warmup_epochs: 2
batch_size: 16
mini_batch: 4
lr: 0.001
num_workers: 2
hidden_dim: 100
depth: 3
n_layers: 2
optimizer: adam
scheduler: plateau
verbose: False

Model parameters are:
node_dim: 26
edge_dim: 2
hidden_dim: 100
depth: 3
n_layers: 2


Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Scheduler state dict is:
factor: 0.7
min_lrs: [1e-05]
patience: 5
verbose: False
cooldown: 0
cooldown_counter: 0
mode: min
threshold: 0.0001
threshold_mode: rel
best: inf
num_bad_epochs: 0
mode_worse: inf
eps: 1e-08
last_epoch: 0

Starting training...
Epoch 1: Training Loss 0.08688119150258738
Epoch 1: Validation Loss 0.07419156318639918
Epoch 2: Training Loss 0.07425958234616961
Epoch 2: Validation Loss 0.07667599137665582
Epoch 3: Training Loss 0.07359444461338464
Epoch 3: Validation Loss 0.08172006867462445
Epoch 4: Training Loss 0.07264292819981472
Epoch 4: Validation Loss 0.07071985567008555
Epoch 5: Training Loss 0.07085340066313538
Epoch 5: Validation Loss 0.07222099346960163
Epoch 6: Training Loss 0.07020272642989824
Epoch 6: Validation Loss 0.0751159034621982
Epoch 7: Training Loss 0.06972384961526647
Epoch 7: Validation Loss 0.07124669408472901
Epoch 8: Training Loss 0.0689388540176761
Epoch 8: Validation Loss 0.06967909642184099
