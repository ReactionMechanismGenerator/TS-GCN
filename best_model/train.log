Arguments are...
log_dir: exps/ts_gen_prod
sdf_dir: /home/gridsan/lagnajit/data/intra_gsm_clean
split_path: /home/gridsan/lagnajit/data/intra_gsm_clean/splits/split0.npy
n_epochs: 100
warmup_epochs: 2
batch_size: 16
mini_batch: 4
lr: 0.001
num_workers: 20
hidden_dim: 100
depth: 3
n_layers: 2
optimizer: adam
scheduler: plateau
verbose: False

Model parameters are:
node_dim: 26
edge_dim: 2
hidden_dim: 100
depth: 3
n_layers: 2


Optimizer parameters are:
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)

Scheduler state dict is:
factor: 0.7
min_lrs: [1e-05]
patience: 5
verbose: False
cooldown: 0
cooldown_counter: 0
mode: min
threshold: 0.0001
threshold_mode: rel
best: inf
num_bad_epochs: 0
mode_worse: inf
eps: 1e-08
last_epoch: 0

Starting training...
Epoch 1: Training Loss 0.08677199764290781
Epoch 1: Validation Loss 0.07664040929950115
Epoch 2: Training Loss 0.07646299258323905
Epoch 2: Validation Loss 0.07279026647046426
Epoch 3: Training Loss 0.07479419705543765
Epoch 3: Validation Loss 0.07552525011226112
Epoch 4: Training Loss 0.07362555811066251
Epoch 4: Validation Loss 0.07660933649899034
Epoch 5: Training Loss 0.07193538579558005
Epoch 5: Validation Loss 0.07089271578735282
Epoch 6: Training Loss 0.07092527692128611
Epoch 6: Validation Loss 0.07052822986568304
Epoch 7: Training Loss 0.07049288907632917
Epoch 7: Validation Loss 0.07288204644672279
Epoch 8: Training Loss 0.07005943894901989
Epoch 8: Validation Loss 0.06992645361311303
Epoch 9: Training Loss 0.06953484909651356
Epoch 9: Validation Loss 0.07190130285657646
Epoch 10: Training Loss 0.06854609131350563
Epoch 10: Validation Loss 0.07422849897767314
Epoch 11: Training Loss 0.06827632446267501
Epoch 11: Validation Loss 0.06877205660552994
Epoch 12: Training Loss 0.06774331808782981
Epoch 12: Validation Loss 0.07079940714577905
Epoch 13: Training Loss 0.06788778071102296
Epoch 13: Validation Loss 0.07047731250626725
Epoch 14: Training Loss 0.0679686384686598
Epoch 14: Validation Loss 0.07369370966417207
Epoch 15: Training Loss 0.06685791800155388
Epoch 15: Validation Loss 0.07099410019754118
Epoch 16: Training Loss 0.06762075053660363
Epoch 16: Validation Loss 0.07673949427996064
Epoch 17: Training Loss 0.06672850325146698
Epoch 17: Validation Loss 0.06922097847503633
Epoch 18: Training Loss 0.065172204630831
Epoch 18: Validation Loss 0.06966346582550409
Epoch 19: Training Loss 0.06478746285264075
Epoch 19: Validation Loss 0.06989412664008296
Epoch 20: Training Loss 0.06493930334447168
Epoch 20: Validation Loss 0.07109434855873965
Epoch 21: Training Loss 0.0647241748857827
Epoch 21: Validation Loss 0.07265407876732578
Epoch 22: Training Loss 0.06517471654718737
Epoch 22: Validation Loss 0.07303633175458425
Epoch 23: Training Loss 0.0641859145201329
Epoch 23: Validation Loss 0.0692279248163271
Epoch 24: Training Loss 0.06318179999719667
Epoch 24: Validation Loss 0.07235527740592694
Epoch 25: Training Loss 0.0630837185905833
Epoch 25: Validation Loss 0.06986539074284995
Epoch 26: Training Loss 0.06242033060411912
Epoch 26: Validation Loss 0.06918165890362636
Epoch 27: Training Loss 0.06260628773264112
Epoch 27: Validation Loss 0.06978835326638805
Epoch 28: Training Loss 0.06217373810125934
Epoch 28: Validation Loss 0.0701570836997986
Epoch 29: Training Loss 0.06221872913276938
Epoch 29: Validation Loss 0.07558763869320105
Epoch 30: Training Loss 0.06167434257426501
Epoch 30: Validation Loss 0.06943522988157748
Epoch 31: Training Loss 0.06140740561064558
Epoch 31: Validation Loss 0.07185898292433483
Epoch 32: Training Loss 0.06113545542477226
Epoch 32: Validation Loss 0.06924600269919967
Epoch 33: Training Loss 0.06128779343470046
Epoch 33: Validation Loss 0.07222095551835934
Epoch 34: Training Loss 0.061009108726353
Epoch 34: Validation Loss 0.07574221318460549
Epoch 35: Training Loss 0.060917251376299046
Epoch 35: Validation Loss 0.0728637823894532
Epoch 36: Training Loss 0.06064818799018022
Epoch 36: Validation Loss 0.07552365391298932
Epoch 37: Training Loss 0.060055774028700264
Epoch 37: Validation Loss 0.07299269121074299
Epoch 38: Training Loss 0.05973098777079468
Epoch 38: Validation Loss 0.07537530120049775
Epoch 39: Training Loss 0.05968434105714382
Epoch 39: Validation Loss 0.07429474042502124
Epoch 40: Training Loss 0.060033025089237656
Epoch 40: Validation Loss 0.07221750934603836
Epoch 41: Training Loss 0.059852503017795315
Epoch 41: Validation Loss 0.07537779641562137
Epoch 42: Training Loss 0.05932342037493121
Epoch 42: Validation Loss 0.07362636676340611
Epoch 43: Training Loss 0.05863007107793292
Epoch 43: Validation Loss 0.07524750376865441
Epoch 44: Training Loss 0.058939899744533585
Epoch 44: Validation Loss 0.07302324485036762
Epoch 45: Training Loss 0.05881480571647756
Epoch 45: Validation Loss 0.07544513600025075
Epoch 46: Training Loss 0.058928020989068616
Epoch 46: Validation Loss 0.07635053140537572
Epoch 47: Training Loss 0.05894435847829015
Epoch 47: Validation Loss 0.07282082797244532
Epoch 48: Training Loss 0.05843976725360374
Epoch 48: Validation Loss 0.07644448282716687
Epoch 49: Training Loss 0.058754443586859745
Epoch 49: Validation Loss 0.07986959058387565
Epoch 50: Training Loss 0.058765158855708655
Epoch 50: Validation Loss 0.07544504445834475
Epoch 51: Training Loss 0.05851186594717529
Epoch 51: Validation Loss 0.07480329671463859
Epoch 52: Training Loss 0.05865380754156456
Epoch 52: Validation Loss 0.07822737894701354
Epoch 53: Training Loss 0.05815243440077978
Epoch 53: Validation Loss 0.07684937217116793
Epoch 54: Training Loss 0.0584679796461373
Epoch 54: Validation Loss 0.07745496315717512
Epoch 55: Training Loss 0.058166402289639034
Epoch 55: Validation Loss 0.07717199240385067
Epoch 56: Training Loss 0.058316101566760634
Epoch 56: Validation Loss 0.07834670120815534
Epoch 57: Training Loss 0.058094295248725725
Epoch 57: Validation Loss 0.07785359343062165
Epoch 58: Training Loss 0.0578665748856691
Epoch 58: Validation Loss 0.07762827363062319
Epoch 59: Training Loss 0.05849316575591597
Epoch 59: Validation Loss 0.07713709812449564
Epoch 60: Training Loss 0.05781480866897602
Epoch 60: Validation Loss 0.07666029571484041
Epoch 61: Training Loss 0.058102695382697474
Epoch 61: Validation Loss 0.07734721304457372
Epoch 62: Training Loss 0.0578638338795615
Epoch 62: Validation Loss 0.0784701500326914
Epoch 63: Training Loss 0.05740443422141505
Epoch 63: Validation Loss 0.07806428197237363
Epoch 64: Training Loss 0.058197326795539385
Epoch 64: Validation Loss 0.07916200486509425
Epoch 65: Training Loss 0.057509979794997175
Epoch 65: Validation Loss 0.07568963532173544
Epoch 66: Training Loss 0.057867873410544786
Epoch 66: Validation Loss 0.07933126807924047
Epoch 67: Training Loss 0.057591950466876485
Epoch 67: Validation Loss 0.08187852187423131
Epoch 68: Training Loss 0.05748163957062984
Epoch 68: Validation Loss 0.07708552956286534
Epoch 69: Training Loss 0.05785284773737659
Epoch 69: Validation Loss 0.07792169787502243
Epoch 70: Training Loss 0.057762668690592704
Epoch 70: Validation Loss 0.076823337431097
Epoch 71: Training Loss 0.057809326716557495
Epoch 71: Validation Loss 0.07660007371705484
Epoch 72: Training Loss 0.05752677026533165
Epoch 72: Validation Loss 0.07961136039139272
Epoch 73: Training Loss 0.05753061948170787
Epoch 73: Validation Loss 0.08064687759111103
Epoch 74: Training Loss 0.057715878344437886
Epoch 74: Validation Loss 0.07621251871369883
Epoch 75: Training Loss 0.0574286495925993
Epoch 75: Validation Loss 0.07744906783385673
Epoch 76: Training Loss 0.057763880490011105
Epoch 76: Validation Loss 0.07945124778088838
Epoch 77: Training Loss 0.05769973408595487
Epoch 77: Validation Loss 0.08056228573341177
Epoch 78: Training Loss 0.05761566464088923
Epoch 78: Validation Loss 0.07768322601617456
Epoch 79: Training Loss 0.057372283694375664
Epoch 79: Validation Loss 0.07854018583372577
Epoch 80: Training Loss 0.05778310225710994
Epoch 80: Validation Loss 0.08007741392631096
Epoch 81: Training Loss 0.05770379497342805
Epoch 81: Validation Loss 0.08190723690215508
Epoch 82: Training Loss 0.05776292417838637
Epoch 82: Validation Loss 0.08044895330477439
Epoch 83: Training Loss 0.05765896744047696
Epoch 83: Validation Loss 0.07994764715098002
Epoch 84: Training Loss 0.057288981682216855
Epoch 84: Validation Loss 0.07875704157239609
Epoch 85: Training Loss 0.057064773690079916
Epoch 85: Validation Loss 0.07899502374538406
Epoch 86: Training Loss 0.05767185828287628
Epoch 86: Validation Loss 0.0788100487103219
Epoch 87: Training Loss 0.057421883329386324
Epoch 87: Validation Loss 0.07704127171408257
Epoch 88: Training Loss 0.05727564570528391
Epoch 88: Validation Loss 0.08025966266636622
Epoch 89: Training Loss 0.05740678529933399
Epoch 89: Validation Loss 0.0799893542499594
Epoch 90: Training Loss 0.05732474053910736
Epoch 90: Validation Loss 0.07748959168605343
Epoch 91: Training Loss 0.05739709129334446
Epoch 91: Validation Loss 0.08003056904576983
Epoch 92: Training Loss 0.05730496332298284
Epoch 92: Validation Loss 0.07872448849619085
Epoch 93: Training Loss 0.05745297370672567
Epoch 93: Validation Loss 0.07759372714906804
Epoch 94: Training Loss 0.05733982387441737
Epoch 94: Validation Loss 0.07774480757595335
Epoch 95: Training Loss 0.05758353386194608
Epoch 95: Validation Loss 0.08103246334764908
Epoch 96: Training Loss 0.05726065173731395
Epoch 96: Validation Loss 0.07866862581194743
Epoch 97: Training Loss 0.05727186098283295
Epoch 97: Validation Loss 0.07821494975905348
Epoch 98: Training Loss 0.05751641826957202
Epoch 98: Validation Loss 0.07941451794054717
Epoch 99: Training Loss 0.05692995239706961
Epoch 99: Validation Loss 0.07840616978973815
Best Validation Loss 0.06877205660552994 on Epoch 11
